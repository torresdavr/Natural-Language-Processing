{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with Text Data in Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agenda:\n",
    "1. Model Building in Scikit-learn - object shapes\n",
    "2. Representing text as numerical data\n",
    "3. Reading SMS data\n",
    "4. Vectorizing the SMS data\n",
    "5. Building a Naive Bayes Model\n",
    "6. Comparing Naive Bayes with Logistic Regression\n",
    "7. Calculating the 'spaminess' of each token\n",
    "8. Creating a dataframe from individual text files\n",
    "\"\"\"\n",
    "\n",
    "#iris is classification dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Model Building in Scikit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Features are also known as predictors, inputs, or attributes.\n",
    "The response is also known as target, label, output\n",
    "\n",
    "store the feature matrix (X) and response vector (y)\n",
    "\"\"\"\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine 5 rows of X, including feature names\n",
    "pd.DataFrame(X, columns=iris.feature_names).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the class\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#instantiate the model with default parameters\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#fit the model with the data\n",
    "knn.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict response for new observation\n",
    "knn.predict([[3,5,4,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Representing Text as Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example test for model training (SMS messages)\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me...Please!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call you tonight', 'Call me a cab', 'please call me...Please!']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and instantiate Countvectorizer(with default parameters)\n",
    "#Countvectorizer is used for extracintg deafures from text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learn the vocabulary of the training data.  Learns the vocabulary, what are words being used\n",
    "vect.fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab', 'call', 'me', 'please', 'tonight', 'you']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "examine the fitted vocabulary it learned\n",
    "returns python list of strings\n",
    "fitted vocabulary\n",
    "put in alphabetical order\n",
    "Lowercase=True - puts all lowercase\n",
    "no capitals(ignores case, no duplicates, no punctuation)\n",
    "deletes anything that doesn't have at least 2 characters, not removing stop words(didn't remove 'a' b/c stop word, it's less\n",
    "than 2 character)\n",
    "-no duplicates. Didn't put same word in twice\n",
    "\"\"\"\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform training data into document-term matrix\n",
    "simple_train_dtm = vect.transform(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "3 documents, 6 vocabulary words(terms, features, tokens)\n",
    "\"\"\"\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 2, 0, 0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documens matrices have to array method\n",
    "#convert sparse matrix to dense matrix\n",
    "# 3 x 6 matrix as a dense matrix\n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "examine the vocabulary and document term matrix together\n",
    "It took text which was non-numeric and variable length and it's now representing it\n",
    "     as a feature matrix with fixed number of columns\n",
    "It's just a count of the number of times a token appears in that message\n",
    "\n",
    "feature matrix. Create this b/c you need an X.\n",
    "\"\"\"\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "features and samples defined as follows:\n",
    "-each individual token occurence frequency(normalized or not) is treated as a feature\n",
    "- The vector of all token frequencies for a given document is considered a multivariate sample.\n",
    "    A corpus of documents can be represented as a matrix, with 1 row per document, 1 column per token/word/feature.\n",
    "    We have a corpus of 3 documents.\n",
    "vectorization-process of turning a collection of text documents into numerical feature vectors. This strategy(a/k/a tokenization,\n",
    "counting, normailization) is called bag of words. Documents are described by word occurences while ignoring the realtive \n",
    "position information of words in the document. Ordering is lost.\n",
    "\n",
    "Count of the number of times a token appears in that message\n",
    "sparse matrix doesn't store locations of zeros, stores locations of nonzeros.\n",
    "    ie, (2,3)  2    location, go down to row 2, over 0,1,2,3 has a 2\n",
    "    Doesn't tell you where zeros are, only non-zeros.\n",
    "print sparse matrix\n",
    "\"\"\"\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "point of building a model is to make predictions later.\n",
    "I'm going to have new text data that comes in, what do I do with it?\n",
    "\n",
    "-example text for model testing.\n",
    "\"\"\"\n",
    "simple_test = [\"please don't call me\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform testing data into a document-term matrix (using existing vocabulary)\n",
    "simple_test_dtm = vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "examine the vocabulary and document-term matrix together\n",
    "-what has happened that isn't optimal?\n",
    "what happened to 'don't'.  'don't' wasn't a feature previously.  Wasn't a token in training data so it get dropped.\n",
    "It's ok it dropped.\n",
    "Since it wasn't in original corpus it won't assist in modeling text.\n",
    "\n",
    "in spam case, 'don't' may be highly relevant to predicting ham or spam, but if that word has never been seen in our \n",
    "    training data, the model doesn't have any reason to believe to it's related to spam or non-spam.Training model never \n",
    "    learned whether 'don't' is a hammy or spammy word. It's dropped b/c we wouldn't learn anything from it.\n",
    "\n",
    "vect.fit(train) - learns the vocabulary of hte training data    \n",
    "vect.transform(train) - uses the fitted vocabulary to build a document term matrix from the training data    \n",
    "vect.transform.(test) -  uses the fitted vocabulary to build a document term matrix from the testing data (and\n",
    "    ignores tokens it hasn't seen before)\n",
    "\n",
    "-we didn't run fit on the testing data. We ran fit and transform on training data. If we had run a fit on the testing data we would have \n",
    "    learned a new vocabulary of 4 words, it would not match the vocalubalry of 6 words that was in the training data.\n",
    "\n",
    "\"\"\"\n",
    "pd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Reading the SMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/sms.tsv'\n",
    "sms = pd.read_table(path, header=None, names=['label','message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine the class distribution\n",
    "# ham is word for non-spam\n",
    "sms.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#want to convert label in your response to a numerical variable\n",
    "sms['label_num'] = sms.label.map({'ham':0, 'spam':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3   ham  U dun say so early hor... U c already then say...          0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...          1\n",
       "6   ham  Even my brother is not like to speak with me. ...          0\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...          0\n",
       "8  spam  WINNER!! As a valued network customer you have...          1\n",
       "9  spam  Had your mobile 11 months or more? U R entitle...          1"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "#usual way t odefine X and y (from iris data) for use with a model\n",
    "X= iris.data\n",
    "y = iris.target\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572,)\n",
      "(5572,)\n"
     ]
    }
   ],
   "source": [
    "#required way to define X and y for use with convectorizor\n",
    "#both are 1D. Starting with X as 1D b/c it will get converted by countvectorizer into a 2d.\n",
    "X = sms.message  #message series as X data\n",
    "y = sms.label_num  # label_num series as y\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179,)\n",
      "(4179,)\n",
      "(1393,)\n",
      "(1393,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "split X and y into training and testing sets before vectorization\n",
    "the point of train test split is you believe the test set you're making is a simulation of the future.\n",
    "Reality is that future docs will have words you've never seen before. Models are handicapped by fact taht they can only\n",
    "    learn from the past.\n",
    "If you vectorize before splitting you model, your testing set will not see any words that it didn't learn already.\n",
    "Making it too easy for model if you vectorize first, then train/test split.\n",
    "\n",
    "by default split is 75/25\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Vectorizing SMS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the vectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn trainig data vocabulary, then use it to create a document-term matrix\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4179x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 55209 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#examine document term matrix\n",
    "#4179 sms messages, training documents and 7456 individual words(unique tokens. duplicate words only get counted once)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1393x7456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17604 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform testing data (using fitted vocabulary)into document-term matix\n",
    "#7456 will be the same b/c transfom is using the fitted vocabulary and it learned a vocabulary of 7456 previously.\n",
    "#7456 is hte number of features, tokens taht were learned in the fit step.\n",
    "#knew it was goin to be 1393 b/c taht is hte number of test documents(X_test.shape)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 5: Building and Evaluating Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import and substantiate Multinomial Naive Bayes Model\n",
    "There a different types of naive bayes we're using multinomial\n",
    "Multinomial Naive Bayes is suitable for Classification with discrete features(meaning integer features),such as word count.\n",
    "Won't accept negative numbers as features.\n",
    "Multinomial is very common to use for text problems\n",
    "\"\"\"\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.53 ms, sys: 2.39 ms, total: 7.92 ms\n",
      "Wall time: 7.08 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using X_train_dtm(timing it with 'magic command')\n",
    "%time nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make class predicitons for X_test_dtm, not X_test\n",
    "#Step 4, make a prediction\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9885139985642498"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Accuracy is percent correct.\n",
    "calculate accuracy of class predictions\n",
    "we're using roughly 4000 ham/spam messages and short text to predict whether another 1300 messages are ham or spam \n",
    "\n",
    "pass acual values first, predicted values 2nd.\n",
    "\n",
    "null accuracy- our null accuracy is hte the accuracy that could be accomplished by always predicting the majority class\n",
    "   That would 75 or 80% b/c of imbalcned classes.  We have 3/4 ham and the rest is spam.\n",
    "   If we always predcited ham we would get it iright 75% of the time.\n",
    "Predicting ham or spam.  Based on words in a message.\n",
    "\"\"\"\n",
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1203,    5],\n",
       "       [  11,  174]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print the confusion matrix\n",
    "Looking for predictions for the testing set.\n",
    "#comparing actuals v. predicted\n",
    "TN = 1203, this was a correct prediction\n",
    "TP = 174, this was a correct prediction\n",
    "FP = 5, false positives. Means message was incorrectly classified as spam. Predicted spam and it was ham.\n",
    "FN = 11, false negatives\n",
    "\"\"\"\n",
    "metrics.confusion_matrix(y_test, y_pred_class )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574               Waiting for your call.\n",
       "3375             Also andros ice etc etc\n",
       "45      No calls..messages..missed calls\n",
       "3415             No pic. Please re-send.\n",
       "1988    No calls..messages..missed calls\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print message text for the false positives (meaning they were incorrectly classified as spam)\n",
    "classifier incorrectly predicted spam\n",
    "for any case where the prediction is 1, and the acutal is 0.It returns a True and that Series of True and False is passed\n",
    "    to X_test to select out rows.\n",
    "These are the 5 messages that were hand marked as ham, meaning person that built is said these are ham messages and the \n",
    "    classifier incorrectly predicted spam.\n",
    "\"\"\"\n",
    "X_test[y_test < y_pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3132    LookAtMe!: Thanks for your purchase of a video...\n",
       "5       FreeMsg Hey there darling it's been 3 week's n...\n",
       "3530    Xmas & New Years Eve tickets are now on sale f...\n",
       "684     Hi I'm sue. I am 20 years old and work as a la...\n",
       "1875    Would you like to see my XXX pics they are so ...\n",
       "1893    CALL 09090900040 & LISTEN TO EXTREME DIRTY LIV...\n",
       "4298    thesmszone.com lets you send free anonymous an...\n",
       "4949    Hi this is Amy, we will be sending you a free ...\n",
       "2821    INTERFLORA - Â“It's not too late to order Inter...\n",
       "2247    Hi ya babe x u 4goten bout me?' scammers getti...\n",
       "4514    Money i have won wining number 946 wot do i do...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print message text fo false negatives(meaning they were incorrectly classified as ham)\n",
    "#false negative is a spam message that was incorrectly classified as ham\n",
    "X_test[y_test > y_pred_class]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1078    0\n",
       "4028    0\n",
       "958     0\n",
       "4642    0\n",
       "4674    0\n",
       "5461    0\n",
       "4210    0\n",
       "4216    0\n",
       "1603    0\n",
       "1504    0\n",
       "1783    0\n",
       "3465    0\n",
       "5534    0\n",
       "4267    0\n",
       "2498    0\n",
       "4259    0\n",
       "147     1\n",
       "141     0\n",
       "4517    1\n",
       "3053    0\n",
       "5392    0\n",
       "2346    0\n",
       "1242    0\n",
       "3224    0\n",
       "4872    0\n",
       "3044    0\n",
       "1660    0\n",
       "3214    0\n",
       "501     0\n",
       "1827    0\n",
       "       ..\n",
       "2285    0\n",
       "4829    0\n",
       "2155    0\n",
       "3555    0\n",
       "4582    0\n",
       "1010    0\n",
       "2007    0\n",
       "221     0\n",
       "1961    1\n",
       "1141    0\n",
       "2270    0\n",
       "2589    0\n",
       "3779    0\n",
       "1714    1\n",
       "1463    1\n",
       "2694    0\n",
       "1925    0\n",
       "3597    0\n",
       "1988    0\n",
       "932     0\n",
       "2870    0\n",
       "5458    0\n",
       "2890    0\n",
       "3658    0\n",
       "4285    0\n",
       "3207    0\n",
       "4655    0\n",
       "1140    0\n",
       "1793    1\n",
       "1710    0\n",
       "Name: label_num, Length: 1393, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LookAtMe!: Thanks for your purchase of a video clip from LookAtMe!, you've been charged 35p. Think you can do better? Why not send a video in a MMSto 32323.\""
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "what do you notice about the false negatives\n",
    "false negatives are a lot longer\n",
    "Naive Bayes is getting lost in hammy words.\n",
    "\"\"\"\n",
    "X_test[3132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "predicted probabilties-what is the predicted probability of class membership for each of the observations. \n",
    "    What is the model's prediction of the liklihood that it is spam or ham.\n",
    "calculate predicted probabilites for X_test_dtm (poorly calibrated)\n",
    "for each of hte obervations what is hte models prediction of likelyhood that it is spam\n",
    "predicted probability of class 1 which is spam [:,1]. \n",
    "[:,1] I want all rows and I want column 1. Column 1 is the predicted probability of class 1.  Class 1 is 'spam'.\n",
    "\"\"\"\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.87744864e-03, 1.83488846e-05, 2.07301295e-03, ...,\n",
       "       1.09026171e-06, 1.00000000e+00, 3.98279868e-09])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for the first message in test set, it thinks the likelyhood that it is spam is .0028.\n",
    "last message, it thinks the liklihood that it is spam is almost zero.\n",
    "1.00, it thinks it's 100% spam.\n",
    "These were the predicted probabilities that were output.\n",
    "Naive Bayes has poorly calibrated predicted probabilities. It gives very extreme numbers. Not terribly usefule as predcited \n",
    "    probabilties. If you want t ointerpret as liklihoods, they're not terribly useful.\n",
    "    \n",
    "You may want your model to output predicted probabilitites that can be interpreted as probabilties of class 1, so\n",
    "   I want my model to tell me if it has 60% chance of belonging to class 1.  Maybe I only have time to call 10 prospects per day.\n",
    "   NB prections should not be interpreted as actual probabilties. \n",
    "\"\"\"\n",
    "\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.87744864e-03, 1.83488846e-05, 2.07301295e-03, ...,\n",
       "       1.09026171e-06, 1.00000000e+00, 3.98279868e-09])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "need t oslice to get predcited probability of class 1.\n",
    "all rows, column 1\n",
    "\"\"\"\n",
    "nb.predict_proba(X_test_dtm)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866431000536962"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Despite the fact that predicted probabilties were so wacky, the AUC is still very high b/c the accuracy was so high already.\n",
    "    No way AUC could be much lower.\n",
    "AUC figure is still great.\n",
    "Use of predicted probabilties:\n",
    "    1)Some evaluation metrics require predicted probabilties.\n",
    "      AUC requires predicted probabilties.Logloss requires predcited probabilties\n",
    "    2) maybe you actually don't care about class predictions\n",
    "      ie, credit card fraud, is this transaction fraudulent.Might not care about accuracy-did I get it right or wrong.\n",
    "      Did predcited probabiltiy break 50% or not. Might say,all I care if something has more than 10% liklihood of being \n",
    "      fraud, then I will flag and it and disallow payment until customer authorizes it.\n",
    "      with fraud want predcited probabilties finely tuned.Care whether it predcits 2% or 12% fraud.\n",
    "    \n",
    "-calculate AUC(Area under the Curve)\n",
    "\"\"\"\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Comparing Naive Bayes with Logistic Regression\n",
    "\n",
    "Compare Naive Bayes with Logitic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import and instantiate logistric regression model\n",
    "# this is a classificaiton model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 61.1 ms, sys: 3.61 ms, total: 64.7 ms\n",
      "Wall time: 35.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "wall time is bigger. Before wall time was Wall time: 20.8 ms\n",
    "Naive Bayes is really fast. Logsitic Regression is slower than Naive Bayes.\n",
    "\"\"\"\n",
    "#train the model using X_test_dtm\n",
    "%time logreg.fit(X_train_dtm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a class prediction for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "calculate predicted probabilities for X_test_dtm( well calibrated). They are well calibrated probabilties.\n",
    "logistic regression probabilities are much more likely to be interpretable as actual liklihoods\n",
    "logistic regression probabilities are better calibrated\n",
    "-Logistic Regression is good if you care about predcited probabilities.\n",
    "   But Logistic Regression is slower than Naive Bayes.\n",
    "-We don't take Naives Bayes probabilties very seriously. Logistic Regression predcited probabilties are better calibrated.\n",
    "\"\"\"\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01269556, 0.00347183, 0.00616517, ..., 0.03354907, 0.99725053,\n",
       "       0.00157706])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9877961234745154"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate accuracy\n",
    "metrics.accuracy_score(y_test,y_pred_class )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9936817612314301"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Both accuracy and roc_auc_score did really well in this case.\n",
    "calculate AUC\n",
    "Minor differences\n",
    "Point-Logistic Regression gives better predicted probabilties, Naives Bayes is faster. 2 ways to think about usage of the \n",
    "    2 models.\n",
    "\"\"\"\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Calculate the spaminess of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7456"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "-get insight from your models.\n",
    "#Can do this with Naive Bayes in particular\n",
    "-Why did certain messages get flagged as spam v ham? \n",
    "-Why question. Were the individual words thought of by the model as like spammy words?\n",
    "-store the vocabulary of X_train as an object.\n",
    "-length is 7456, the vocabulary size. It learned 4179 x 7456, was the size of matrix b/c 7456 is the vocabulary size.\n",
    "\"\"\"\n",
    "X_train_tokens = vect.get_feature_names()\n",
    "len(X_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '008704050406', '0121', '01223585236', '01223585334', '0125698789', '02', '0207', '02072069400', '02073162414', '02085076972', '021', '03', '04', '0430', '05', '050703', '0578', '06', '07', '07008009200', '07090201529', '07090298926', '07123456789', '07732584351', '07734396839', '07742676969', '0776xxxxxxx', '07781482378', '07786200117', '078', '07801543489', '07808', '07808247860', '07808726822', '07815296484', '07821230901', '07880867867', '0789xxxxxxx', '07946746291', '0796xxxxxx', '07973788240', '07xxxxxxxxx', '08', '0800', '08000407165', '08000776320', '08000839402', '08000930705']\n"
     ]
    }
   ],
   "source": [
    "#examine the fist 50 tokens\n",
    "print(X_train_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yer', 'yes', 'yest', 'yesterday', 'yet', 'yetunde', 'yijue', 'ym', 'ymca', 'yo', 'yoga', 'yogasana', 'yor', 'yorge', 'you', 'youdoing', 'youi', 'youphone', 'your', 'youre', 'yourjob', 'yours', 'yourself', 'youwanna', 'yowifes', 'yoyyooo', 'yr', 'yrs', 'ything', 'yummmm', 'yummy', 'yun', 'yunny', 'yuo', 'yuou', 'yup', 'zac', 'zaher', 'zealand', 'zebra', 'zed', 'zeros', 'zhong', 'zindgi', 'zoe', 'zoom', 'zouk', 'zyada', 'Ã¨n', 'ã€¨ud']\n"
     ]
    }
   ],
   "source": [
    "#examine the last 50 tokens\n",
    "print(X_train_tokens[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  1.,  1.],\n",
       "       [ 5., 23.,  2., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#naive bayes, when you run a fit,counts the number of times each token appears in a class\n",
    "# ham is class 0 and spam is class 1\n",
    "\n",
    "#can use to help us tell which words are hammy and which are spammy\n",
    "\n",
    "\"\"\"\n",
    "Above, it kept the vocabulary in alphabetical order.\n",
    "\n",
    "ham is class 0\n",
    "spam to class 1\n",
    "2 rows represent 2 classes, class 0 are ham and class 1 is spam. The 7456 columns represent the features/tokens.\n",
    "0 corresponds to numerically smallest class\n",
    "-regarding above token '00', was found 0 times in ham messages and 5 times in spam messages.\n",
    "-'ã€¨ud', appeared 1 time in ham message and 0 times spam.\n",
    "\n",
    "Can use this data to decide which words are hammy and which words are spammy. A words that appears a lot in ham but not in \n",
    "    spam, it's a good predcitor of a ham message. \n",
    "How do we know which row is ham and which row is spam?  Kevin assigned ham to class 0 and spam to class 1.\n",
    "\n",
    "Just like confusion matrix, it's sorted with the lowest class numerically as hte closest to the upper left corner.\n",
    "\"\"\"\n",
    "nb.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7456)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rows represent classes and columns represent tokens\n",
    "#2 rows represent 2 classes: class 0 is ham and class 1 is spam\n",
    "#7456 represent the features, represnts tokens\n",
    "#first token 00 was found 0 times in ham messages, but 5 times in spam messages\n",
    "# the last token appeared 1 time inf ham message and o times in spam message.\n",
    "#numpy array\n",
    "#a word that appears a lot in ham but not in spam it's a good predictor of a ham message.\n",
    "# 2 x 7456 numpy array\n",
    "nb.feature_count_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first row\n",
    "##number of times each token appears accross all spam messages\n",
    "ham_token_count = nb.feature_count_[0,:]\n",
    "ham_token_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5., 23.,  2., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#second row\n",
    "#number of times each token appears accross all spam messages\n",
    "spam_token_count = nb.feature_count_[1,:]\n",
    "spam_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe of tokens with their separate ham and spam counts\n",
    "tokens = pd.DataFrame({'token':X_train_tokens, 'ham':ham_token_count, 'spam':spam_token_count}).set_index('token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008704050406</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0121</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01223585236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01223585334</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0125698789</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0207</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02072069400</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02073162414</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>02085076972</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>021</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>03</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0430</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>05</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>050703</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0578</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07008009200</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07090201529</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07090298926</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07123456789</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07732584351</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07734396839</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07742676969</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0776xxxxxxx</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07781482378</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yourjob</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yours</th>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yourself</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youwanna</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yowifes</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yoyyooo</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yr</th>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yrs</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ything</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yummmm</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yummy</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yun</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yunny</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuo</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuou</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yup</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zac</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zaher</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealand</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zebra</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zed</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeros</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zhong</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zindgi</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoe</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoom</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zouk</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zyada</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ã¨n</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ã€¨ud</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7456 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ham  spam\n",
       "token                   \n",
       "00             0.0   5.0\n",
       "000            0.0  23.0\n",
       "008704050406   0.0   2.0\n",
       "0121           0.0   1.0\n",
       "01223585236    0.0   1.0\n",
       "01223585334    0.0   2.0\n",
       "0125698789     1.0   0.0\n",
       "02             0.0   4.0\n",
       "0207           0.0   3.0\n",
       "02072069400    0.0   1.0\n",
       "02073162414    0.0   2.0\n",
       "02085076972    0.0   1.0\n",
       "021            0.0   2.0\n",
       "03             0.0   6.0\n",
       "04             0.0   9.0\n",
       "0430           0.0   1.0\n",
       "05             0.0   1.0\n",
       "050703         0.0   1.0\n",
       "0578           0.0   1.0\n",
       "06             0.0   3.0\n",
       "07             0.0   2.0\n",
       "07008009200    0.0   1.0\n",
       "07090201529    0.0   1.0\n",
       "07090298926    0.0   1.0\n",
       "07123456789    0.0   1.0\n",
       "07732584351    0.0   1.0\n",
       "07734396839    0.0   2.0\n",
       "07742676969    0.0   2.0\n",
       "0776xxxxxxx    0.0   2.0\n",
       "07781482378    0.0   1.0\n",
       "...            ...   ...\n",
       "yourjob        1.0   0.0\n",
       "yours          4.0  12.0\n",
       "yourself      13.0   1.0\n",
       "youwanna       1.0   0.0\n",
       "yowifes        1.0   0.0\n",
       "yoyyooo        1.0   0.0\n",
       "yr             2.0   9.0\n",
       "yrs            3.0   3.0\n",
       "ything         1.0   0.0\n",
       "yummmm         1.0   0.0\n",
       "yummy          3.0   0.0\n",
       "yun            2.0   0.0\n",
       "yunny          2.0   0.0\n",
       "yuo            4.0   0.0\n",
       "yuou           1.0   0.0\n",
       "yup           33.0   0.0\n",
       "zac            1.0   0.0\n",
       "zaher          1.0   0.0\n",
       "zealand        1.0   0.0\n",
       "zebra          0.0   1.0\n",
       "zed            0.0   6.0\n",
       "zeros          1.0   0.0\n",
       "zhong          1.0   0.0\n",
       "zindgi         2.0   0.0\n",
       "zoe            1.0   1.0\n",
       "zoom           1.0   0.0\n",
       "zouk           0.0   1.0\n",
       "zyada          1.0   0.0\n",
       "Ã¨n             1.0   0.0\n",
       "ã€¨ud            1.0   0.0\n",
       "\n",
       "[7456 rows x 2 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>64.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasty</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>villa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beloved</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textoperator</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ham  spam\n",
       "token                   \n",
       "very          64.0   2.0\n",
       "nasty          1.0   1.0\n",
       "villa          0.0   1.0\n",
       "beloved        1.0   0.0\n",
       "textoperator   0.0   2.0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>65.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasty</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>villa</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beloved</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textoperator</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ham  spam\n",
       "token                   \n",
       "very          65.0   3.0\n",
       "nasty          2.0   2.0\n",
       "villa          1.0   2.0\n",
       "beloved        2.0   1.0\n",
       "textoperator   1.0   3.0"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "want to calculate for every word a spaminess score.\n",
    "Going to use the ratios of spam to ham inorder to do that now.\n",
    "2 things to do:\n",
    "1. account for class imbalance that we were just talking about.  We need spam to have a higher weight\n",
    "Word appearing in spam has to have a higher weight b/c of class imbalance.\n",
    "2. make sure there are no /0 errors\n",
    "If I'm doing a ratio nof 1 column to another, having a 0 is going to be a problem.  I'm going to add 1 to both columns\n",
    "\"\"\"\n",
    "tokens['ham'] = tokens.ham + 1\n",
    "tokens['spam'] = tokens.spam + 1\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3617.,  562.])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "I want to normalize numbers\n",
    "I want to divide by the number of spam messages and ham.  So I can get frequencies rather than counts\n",
    "\n",
    "naive bayes counts hte number of observations in each class\n",
    "3617 ham and 562 spam\n",
    "\"\"\"\n",
    "nb.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.005338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasty</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.003559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>villa</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.003559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beloved</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textoperator</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.005338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ham      spam\n",
       "token                           \n",
       "very          0.017971  0.005338\n",
       "nasty         0.000553  0.003559\n",
       "villa         0.000276  0.003559\n",
       "beloved       0.000553  0.001779\n",
       "textoperator  0.000276  0.005338"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "It's a better measure of raw counts b/c it's adjusted for class imbalance.\n",
    "ie, 'nasty' is less prevalent in spam messages than ham messages, 0.000553 vs. 0.003559\n",
    "Don't take figures too seriously w/o accounting for fact that you added 1 before I did division.\n",
    "convert ham and spam counts to frequencies\n",
    "\"\"\"\n",
    "tokens['ham'] = tokens.ham / nb.class_count_[0]\n",
    "tokens['spam'] = tokens.spam / nb.class_count_[1]\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.005338</td>\n",
       "      <td>0.297044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasty</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>6.435943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>villa</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>12.871886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beloved</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>3.217972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textoperator</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.005338</td>\n",
       "      <td>19.307829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ham      spam  spam_ratio\n",
       "token                                       \n",
       "very          0.017971  0.005338    0.297044\n",
       "nasty         0.000553  0.003559    6.435943\n",
       "villa         0.000276  0.003559   12.871886\n",
       "beloved       0.000553  0.001779    3.217972\n",
       "textoperator  0.000276  0.005338   19.307829"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Our most spammy word is 'text operator', followed by 'villa', followed by 'nasty',followed by 'beloved', followed by 'very'.\n",
    "Out of 5 words, 'very' is most 'hammy' word. It's the most predcitive of ham.\n",
    "\n",
    "Naive Bayes learns this ratio. NB learns spam ratio column and uses it to make predicitons.\n",
    "Based on this training dat, if I got a new text message that said 'very nasty villa', what Naive Bayes does is it \n",
    "    learned these ratios, would take the log of those numbers and add them up. Based upon either adding or mutliplying them,\n",
    "    that would be how it decides whether a message is ham or spam.\n",
    "    \n",
    "-calculate for every word the ratio of spam to ham for each token\n",
    "\"\"\"\n",
    "tokens['spam_ratio'] = tokens.spam / tokens.ham\n",
    "tokens.sample(5, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.158363</td>\n",
       "      <td>572.798932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prize</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.135231</td>\n",
       "      <td>489.131673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150p</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.087189</td>\n",
       "      <td>315.361210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tone</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.085409</td>\n",
       "      <td>308.925267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guaranteed</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.076512</td>\n",
       "      <td>276.745552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.069395</td>\n",
       "      <td>251.001779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.065836</td>\n",
       "      <td>238.129893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>www</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.129893</td>\n",
       "      <td>234.911922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.056940</td>\n",
       "      <td>205.950178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awarded</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.053381</td>\n",
       "      <td>193.078292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150ppm</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.051601</td>\n",
       "      <td>186.642349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uk</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.099644</td>\n",
       "      <td>180.206406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.048043</td>\n",
       "      <td>173.770463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ringtone</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.044484</td>\n",
       "      <td>160.898577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>154.462633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mob</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.042705</td>\n",
       "      <td>154.462633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>co</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.078292</td>\n",
       "      <td>141.590747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>collection</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.039146</td>\n",
       "      <td>141.590747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valid</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.037367</td>\n",
       "      <td>135.154804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.037367</td>\n",
       "      <td>135.154804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.037367</td>\n",
       "      <td>135.154804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10p</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.037367</td>\n",
       "      <td>135.154804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8007</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.035587</td>\n",
       "      <td>128.718861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.067616</td>\n",
       "      <td>122.282918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weekly</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.033808</td>\n",
       "      <td>122.282918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tones</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>115.846975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>land</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>115.846975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.032028</td>\n",
       "      <td>115.846975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.030249</td>\n",
       "      <td>109.411032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.030249</td>\n",
       "      <td>109.411032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>went</th>\n",
       "      <td>0.012718</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.139912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ll</th>\n",
       "      <td>0.052530</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>0.135494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>told</th>\n",
       "      <td>0.013824</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.128719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel</th>\n",
       "      <td>0.013824</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.128719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gud</th>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.126195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos</th>\n",
       "      <td>0.014929</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.119184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>0.090683</td>\n",
       "      <td>0.010676</td>\n",
       "      <td>0.117731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amp</th>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.117017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>something</th>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.117017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sure</th>\n",
       "      <td>0.015206</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.117017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ok</th>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>0.116488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>0.016312</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.109084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>morning</th>\n",
       "      <td>0.016865</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.105507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yeah</th>\n",
       "      <td>0.017694</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.100562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lol</th>\n",
       "      <td>0.017694</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.100562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anything</th>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.099015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>my</th>\n",
       "      <td>0.150401</td>\n",
       "      <td>0.014235</td>\n",
       "      <td>0.094646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doing</th>\n",
       "      <td>0.019077</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.093275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>0.019630</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.090647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask</th>\n",
       "      <td>0.019630</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.090647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>already</th>\n",
       "      <td>0.019630</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.090647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>too</th>\n",
       "      <td>0.021841</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.081468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>0.048936</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>0.072723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>later</th>\n",
       "      <td>0.030688</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.057981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lor</th>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.054084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.054084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.035665</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.049891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.037858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lt</th>\n",
       "      <td>0.064142</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.027741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gt</th>\n",
       "      <td>0.064971</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.027387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7456 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ham      spam  spam_ratio\n",
       "token                                     \n",
       "claim       0.000276  0.158363  572.798932\n",
       "prize       0.000276  0.135231  489.131673\n",
       "150p        0.000276  0.087189  315.361210\n",
       "tone        0.000276  0.085409  308.925267\n",
       "guaranteed  0.000276  0.076512  276.745552\n",
       "18          0.000276  0.069395  251.001779\n",
       "cs          0.000276  0.065836  238.129893\n",
       "www         0.000553  0.129893  234.911922\n",
       "1000        0.000276  0.056940  205.950178\n",
       "awarded     0.000276  0.053381  193.078292\n",
       "150ppm      0.000276  0.051601  186.642349\n",
       "uk          0.000553  0.099644  180.206406\n",
       "500         0.000276  0.048043  173.770463\n",
       "ringtone    0.000276  0.044484  160.898577\n",
       "000         0.000276  0.042705  154.462633\n",
       "mob         0.000276  0.042705  154.462633\n",
       "co          0.000553  0.078292  141.590747\n",
       "collection  0.000276  0.039146  141.590747\n",
       "valid       0.000276  0.037367  135.154804\n",
       "2000        0.000276  0.037367  135.154804\n",
       "800         0.000276  0.037367  135.154804\n",
       "10p         0.000276  0.037367  135.154804\n",
       "8007        0.000276  0.035587  128.718861\n",
       "16          0.000553  0.067616  122.282918\n",
       "weekly      0.000276  0.033808  122.282918\n",
       "tones       0.000276  0.032028  115.846975\n",
       "land        0.000276  0.032028  115.846975\n",
       "http        0.000276  0.032028  115.846975\n",
       "national    0.000276  0.030249  109.411032\n",
       "5000        0.000276  0.030249  109.411032\n",
       "...              ...       ...         ...\n",
       "went        0.012718  0.001779    0.139912\n",
       "ll          0.052530  0.007117    0.135494\n",
       "told        0.013824  0.001779    0.128719\n",
       "feel        0.013824  0.001779    0.128719\n",
       "gud         0.014100  0.001779    0.126195\n",
       "cos         0.014929  0.001779    0.119184\n",
       "but         0.090683  0.010676    0.117731\n",
       "amp         0.015206  0.001779    0.117017\n",
       "something   0.015206  0.001779    0.117017\n",
       "sure        0.015206  0.001779    0.117017\n",
       "ok          0.061100  0.007117    0.116488\n",
       "said        0.016312  0.001779    0.109084\n",
       "morning     0.016865  0.001779    0.105507\n",
       "yeah        0.017694  0.001779    0.100562\n",
       "lol         0.017694  0.001779    0.100562\n",
       "anything    0.017971  0.001779    0.099015\n",
       "my          0.150401  0.014235    0.094646\n",
       "doing       0.019077  0.001779    0.093275\n",
       "way         0.019630  0.001779    0.090647\n",
       "ask         0.019630  0.001779    0.090647\n",
       "already     0.019630  0.001779    0.090647\n",
       "too         0.021841  0.001779    0.081468\n",
       "come        0.048936  0.003559    0.072723\n",
       "later       0.030688  0.001779    0.057981\n",
       "lor         0.032900  0.001779    0.054084\n",
       "da          0.032900  0.001779    0.054084\n",
       "she         0.035665  0.001779    0.049891\n",
       "he          0.047000  0.001779    0.037858\n",
       "lt          0.064142  0.001779    0.027741\n",
       "gt          0.064971  0.001779    0.027387\n",
       "\n",
       "[7456 rows x 3 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "examine the datframe sorted by spam_ratio\n",
    "note: use sort() instead of sort_values for pandas\n",
    "words very spammy in this dataset\n",
    "Our hammy words in this data are much more conversational:'lol','ask','already','she','he'\n",
    "For your problem you may be interested in digging, what words really contributing to my model pre\n",
    "predicting ham or spam. Great for model diagnostics.Explore what your model\n",
    "has learned.\n",
    "-Might want to know what words really contributed to my model predciting ham or spam. Great for model diagnostics.\n",
    "    Great way to explore what model has learned.\n",
    "\"\"\"\n",
    "tokens.sort_values('spam_ratio', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.66725978647686"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "look up the spam ratio for a given token. Easiest way is to use loc method.\n",
    "\n",
    "row/column. 'dating' has a high spam ratio, but not as high as others.\n",
    "Don't interpret this to mean that words above 1 mean hammy and below 1 mean spammy, can't do this b/c of 1 he added.\n",
    "Don't take numbers too seriously.\n",
    "Look at the general scores and the ranking is what you're looking at.\n",
    "Don't take words too seriously, but are helpful to know which words indicate a certain class for your document.\n",
    "\n",
    "Question, where was the threshold set for ham or spam. Threshold was set by default at 50%. You can adjust threshold manually.\n",
    "Question, cna you use Naive Baye for credit card fraud. Yes. It is a classification model. NB is not only for text based data\n",
    "    NB doesn't know what the data means, it doesn't know it came from text.\n",
    "    \n",
    "Kevin points out, Logistic Regression is not universally more accurate than Naive Bayes. With smaller datasets, NB tends to ahve \n",
    "    a better accuracy than logistic regression. With larger datasets, Logistic Regression tends to do better than NB\n",
    "    becaise it has a lower asymtotic error.\n",
    "\"\"\"\n",
    "tokens.loc['dating','spam_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 8: Creating a Text from Individual Text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_txt/ham/email1.txt',\n",
       " 'data_txt/ham/email3.txt',\n",
       " 'data_txt/ham/email5.txt']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "-ie, my text data is stored in a bunch of separate documents.Not stored in a pre-built dataframe. This is the problem \n",
    "    Kevin wants to solve.\n",
    "    Have a bunch of ham and spam emails.\n",
    "\n",
    "You've 5 emails and that's training data.You've dragged and dropped them into\n",
    "    different folders.\n",
    "You've got 3 ham emails and 2 spam emails. ham folder, spam folder.\n",
    "Goal is to take those 5 files and treat each file as its own document and then build a dataframe that\n",
    "looks just like what we've been using from the SMS dataset.\n",
    "Each document is its own file and you want that to turn that into a dataframe.\n",
    "glob comes up with a list file names. So I have something to iterate through.\n",
    "\n",
    "job of glob is just to get file names, not to read files.\n",
    "\n",
    "*.txt  -means get all text documents in folder\n",
    "Builds a list files to work with in python.\n",
    "-use glob to create a list of hsm filenames \n",
    "\"\"\"\n",
    "import glob\n",
    "ham_filenames = glob.glob('data_txt/ham/*.txt') \n",
    "ham_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a ham email.\\nIt has 2 lines.',\n",
       " 'This is another ham email.',\n",
       " 'This is yet another ham email.']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build an empty list.\n",
    "Each of the file names\n",
    "Open that file, after opening it I want to append the complete text of that file to the ham text list.\n",
    "\n",
    "We've not got a python list of 3 strings. Each list element, meaning each string represnts 1 documents, meaning email.\n",
    "    \\n is a new line character.This entire documents, regardless whether it has 2 lines or 2 thousand lines. It gets stored\n",
    "    as 1 string.That way the documents are separated out as list elements.\n",
    "-read the contents of hte ham files into a list (each list element is 1 email)\n",
    "\n",
    "\"\"\"\n",
    "ham_text = []\n",
    "for filename in ham_filenames:\n",
    "    with open(filename) as f:\n",
    "        ham_text.append(f.read())\n",
    "ham_text        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a spam email.', 'This is another spam email.']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "repeat process for spam files.\n",
    "A list of 2 emails. There were only 2 spam emails.\n",
    "\"\"\"\n",
    "import glob\n",
    "spam_filenames = glob.glob('data_txt//spam/*.txt')\n",
    "spam_text = []\n",
    "for filename in spam_filenames:\n",
    "    with open(filename) as f:\n",
    "        spam_text.append(f.read())\n",
    "spam_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Got a list of my 5 documents as a string\n",
    "-combine the ham ans spam lists\n",
    "\"\"\"\n",
    "all_text = ham_text + spam_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a ham email.\\nIt has 2 lines.',\n",
       " 'This is another ham email.',\n",
       " 'This is yet another ham email.',\n",
       " 'This is a spam email.',\n",
       " 'This is another spam email.']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "need a to create a list of labels where ham=0 and spam=1\n",
    "if you have a list wit h1 element in it,like a ), and you multiply it by a number, it makes a list of that length with\n",
    "that number repeated.\n",
    "I wanted to create a list where in the exact same order I have 0 0 0 1 1 to represent ham or spam\n",
    "\n",
    "\"\"\"\n",
    "all_labels = [0] * len(ham_text) + [1] * len(spam_text)\n",
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This is a ham email.\\nIt has 2 lines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>This is another ham email.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>This is yet another ham email.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>This is a spam email.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>This is another spam email.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                message\n",
       "0      0  This is a ham email.\\nIt has 2 lines.\n",
       "1      0             This is another ham email.\n",
       "2      0         This is yet another ham email.\n",
       "3      1                  This is a spam email.\n",
       "4      1            This is another spam email."
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "final step, convert these 2 lists to a dataframe, so I'm passing a dictionary with column headers label and message \n",
    "I'm passing in the data. Passing it to pd dataframe function\n",
    "Now have a dataframe that looks just like what we built in reading in sms.tsv\n",
    "Countvectorizer knows what '\\n' is and it's going to ignore that.\n",
    "\"\"\"\n",
    "pd.DataFrame({'label': all_labels, 'message':all_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
